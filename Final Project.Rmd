---
title: 'Final Project'
author: "Kelsey Gonzalez and Laura Werthmann"
date: "Due Dec 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE,fig.width=5)
```


Load packages
```{r}
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
library(performanceEstimation) # for performance estimation of svm
library(RCurl) # for performance estimation of svm
library(factoextra) #for hopkins statistic
library(clustertend) #for hopkins statistic
library(cluster) # for gower similarity, pam, and diana (a divisive hierarchical method). clara() is also included, which is basically pam by sampling
library(Rtsne) # for t-SNE plot


```

#Data Preprocessing

Review of World Health Organization's Life Expectancy Data taken from [Kaggle](https://www.kaggle.com/kumarajarshi/life-expectancy-who).

```{r}
rm(list=ls())
load("LifeExpectancyData.Rdata")
data$Status = as.factor(data$Status)
data = subset(data,select = -c(`Hepatitis B`))
```

Then we aquire data to include continent values and make our cuboid navigation easier. This will allow us to "roll up" to other country-key table elements.
```{r}
# from https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes
ISO3166 <- read.csv(text = (getURL("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")))
ISO3166$name <- as.character(ISO3166$name)
# They use Eswatini instead of Swaziland, so let's update that & other naming mismatches
ISO3166$name[ISO3166$name == "Moldova, Republic of"] <- "Republic of Moldova"
ISO3166$name[ISO3166$name == "Korea, Republic of"] <- "Republic of Korea"
ISO3166$name[ISO3166$name == "Korea (Democratic People's Republic of)"] <- "Democratic People's Republic of Korea"
ISO3166$name[ISO3166$name == "Congo, Democratic Republic of the"] <- "Democratic Republic of the Congo"
ISO3166$name[ISO3166$name == "North Macedonia"] <- "The former Yugoslav republic of Macedonia"
ISO3166$name[ISO3166$name == "Tanzania, United Republic of"] <- "United Republic of Tanzania"
```

We created a country data frame to make the data join more easily, and then joined them by country("country", "name"), while adding the continent ("region")
```{r}
countries <- as.data.frame(unique(data$Country))
colnames(countries) <- "Country" 
countries$Country <- as.character(countries$Country)

dfcountry <- left_join(x = countries, 
                   y = ISO3166, 
                   by = c("Country" = "name"))


data <- left_join(x = data,
                   y = dfcountry[,c("Country","region")], 
                   by = c("Country" = "Country"))
```

```{r}
data$Life <- data$`Life expectancy`
data$Status = as.factor(data$Status)
data$Country = as.factor(data$Country)

data <- data %>% select(-c(Polio, "percentage expenditure", Measles, `HIV/AIDS`)) 

data2013 <- data %>% filter(Year=="2013") %>% drop_na()

```


#Describe Data set

https://designing-ggplots.netlify.com/#46

# not sure about the, just experimenting

```{r}
load("LifeExpectancyData_3.Rdata")

library(ggrepel)

ten_countries <- 
  data %>% 
  filter(Year == 2013) %>% 
  select(Country) %>% 
  arrange() %>% 
  head()

p1 <- data %>%
  filter(Year == 2013) %>%
  drop_na() %>% 
  mutate(label = ifelse(Country %in% ten_countries,Country,"")) %>%
  ggplot(aes(BMI, `Life expectancy`)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2")

scatter_plot <- p1 + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "BMI",
       y = "life expectancy")
scatter_plot

ten_countries <- 
  data %>% 
  filter(Year == 2013) %>% 
  select(Country) %>% 
  arrange() %>% 
  head()
```

```{r}
p1 <- data %>%
  filter(Year == 2013) %>%
  drop_na() %>% 
  mutate(label = ifelse(Country %in% ten_countries,Country,"")) %>%
  ggplot(aes(`Adult Mortality`, `Life expectancy`)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2")

scatter_plot <- p1 + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "Adult Mortality rate per 1000",
       y = "life expectancy")
scatter_plot
```




```{r}
ten_countries <- 
  data %>% 
  filter(Year == 2013) %>% 
  select(Country) %>% 
  arrange() %>% 
  head()

p1 <- data %>%
  filter(Year == 2013) %>%
  drop_na() %>% 
  mutate(label = ifelse(Country %in% ten_countries,Country,"")) %>%
  ggplot(aes(log(GDP), `Life expectancy`)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2")

scatter_plot <- p1 + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "log(GDP)",
       y = "life expectancy")
scatter_plot

```


=======

```{r, echo=FALSE}
#http://rpubs.com/nicholas_dirienzo/523778
require(reshape2)

data_melted <- data %>%
  filter(Year == 2013) %>% 
  drop_na() %>% 
  select(-Year, -Country, -Status, -region) %>% 
  scale() #make all the variables on the same scale

summary(data_melted)
```

````{r}

data_melted <- melt(as.data.frame(data_melted))

ggplot(data_melted,
       aes(x = value)) +
  geom_histogram() +
  scale_x_continuous(name="Distribution", limits=c(-2, 2)) +
  facet_wrap(~variable)

```



#Descriptive Analyses
## Pam Clustering 


First things first, do our data even have a natural clustering tendency?
the larger the result, the higher clustering tendency

The hopkins statistic on our read dataset shows about a .79 clustering tendency, where in some cases can resemble 90% confidence level.
source: https://www.datanovia.com/en/lessons/assessing-clustering-tendency/
```{r}
#Hopkins, for numerical data only 
# Compute Hopkins statistic for iris dataset:n must be no larger than num of samples
# seed set internally in the function, always give the same result.

install.packages(c("factoextra", "clustertend"))
library(factoextra)
library(clustertend)

data_no_cat <- data2013 %>% select(-Country, -Status, -region)
data_no_cat <- data2013 %>% select(-Country, -Status, -region, -Year) %>% as.matrix()
rownames(data_no_cat) <- data2013$Country

res <- get_clust_tendency(data_no_cat, n=137, graph = TRUE)
res$hopkins_stat
```

```{r}
res$plot
```

The clustering tendency of our random dataset is around .3 when the length between neighbors is 2 or .5, when the length is 1 there's a distancy of .57 - This means the random dataset is uniformly distributed and is not clusterable.
```{r}
df <- data[, -5]

random_df <- apply(df, 2, 
                function(x){runif(length(2), min(1), (max(4)))})
random_df <- as.data.frame(random_df)
# Standardize the data sets

#df <- scale(df)
random_df <- scale(random_df)


res <- get_clust_tendency(random_df, n = nrow(random_df)-1,
                          graph = FALSE)
res$hopkins_stat

```

```{r}
res$plot

```

Our Next try shows the clustering tendency at .19 which is very small; < 0.5, no clustering tendency
```{r}
set.seed(78)
res <- hopkins(data_no_cat, n = 137)
res$H 
```

The same goes for the random dataframe
```{r}
resran <- get_clust_tendency(random_df, n = nrow(random_df)-1,
                          graph = FALSE)
res$hopkins_stat
```


```{r}
library(cluster)
gower_dist <- daisy(data2013[, -1], 
                    metric = "gower",
                    type = list(logratio = "GDP", 
                                logratio = "Population"))


# Check attributes to ensure the correct methods are being used (I = interval, N = nominal)
# Note that despite logratio being called, the type remains coded as "I"

# cleaning it up
gower_mat <- as.matrix(gower_dist)
rownames(gower_mat) <- colnames(gower_mat) <- data2013$Country


```

Calculate Silhouette width for 2 to 10 clusters using PAM - The top cluster options are 2 and 3
```{r}
sil <- c(NA)
for(i in 2:10){
  pam_fit <- pam(gower_mat, diss=TRUE, k=i)
  sil[i] <-pam_fit$silinfo$avg.width
}

plot(1:10, sil,
xlab = "Number of clusters",
ylab = "Silhouette Width",
lines(1:10, sil))
```

review 2 clusters
```{r}
pam_fit <- pam(gower_dist, diss = TRUE, k = 2)

# add cluster labels to the data. We will use result1 later
data_clean <- data.frame(data2013, pam_fit$cluster)

# show clustering results by country
result1 <- data_clean %>% select(Country,pam_fit.cluster)
clusplot(pam_fit)

```

Review 3 clusters
```{r}
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)

# add cluster labels to the data. We will use result1 later
data_clean <- data.frame(data2013, pam_fit$cluster)

# show clustering results by country
result1 <- data_clean %>% select(Country,pam_fit.cluster)
clusplot(pam_fit)
```


group_by cluster and then compute the summary data (means, median, etc) for each cluster
```{r}
data_results <- data_clean %>%
  mutate(cluster = pam_fit.cluster) %>% #add the cluster column
  select(-pam_fit.cluster) %>% 
  group_by(cluster) %>% #group countries by its cluster 
  do(the_summary = summary(.)) #do: summarize by group/cluster,add the_summary column

data_results$the_summary
```
The results suggest:

Cluster 1 has all developing countries (43) with a median Life Expectancy of around 62 years, mean population of 16,790,712 (median=2,117,361), an average of 10 years of schooling and is mainly comprised of countries in Africa, 37 with 4 Asian countries included .

Cluster 2 has all developing countries (64) with a median Life Expectancy of around 73 years, a mean population of 13,616,677 (median=1,149,462), an average of 13 years of schooling and is mainly comprised of countries in Asia, 25 and the Americas, 21, with 5 African countries also included.

Cluster 3 has mostly developed countries (26) and 5 developing countries, with a median Life Expectancy of around 81 years, a mean population of 4,663,832  (median=1,684,432). Cluster 3 has an average of 16 years of schooling and is mainly comprised of countries in Europe, 27, Asia 2, Oceania, 1 and the Americas 1.



One way to visualize many variables in a lower dimensional space is with t-distributed stochastic neighborhood embedding, or t-SNE. 
```{r}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)

tsne_data_pam <- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering),
         name = data_clean$Country)

ggplot(aes(x = X, y = Y), data = tsne_data_pam) +
  geom_point(aes(color = cluster)) + 
  geom_text(data=subset(tsne_data_pam[pam_fit$medoids, ]), # add names of centroids in
            aes(X,Y,label=name))
```
clearly pam isn't the ideal clustering algorithm here. 

## Hierarchical Clustering 

We can use hierarchical clustering to show where the best fit clusters might be.
```{r}
h <- hclust(d = as.dist(gower_dist), method="complete")
plot(h, cex = 0.3, hang = -1, main="Hierarchal Cluster of Predicted Probabilities") +
  abline(h=quantile(gower_dist, .90), col="blue")
```

```{r}
tsne_data_hc <- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(cutree(h, h=quantile(gower_dist, .90))),
         name = data_clean$Country)

set.seed(5)
labels <- tsne_data_hc %>%
    group_by(cluster) %>%
    sample_n(.,1) 

tsne_data_hc <- tsne_data_hc %>% 
  mutate(label = ifelse(name %in% labels$name, as.character(name), ""))

ggplot(aes(x = X, y = Y), data = tsne_data_hc) +
  geom_point(aes(color = cluster)) + 
  geom_text(aes(X,Y,label=label))

```

## Density Clustering with Optics


```{r}
#18 dimensions, so set k = 19

library(dbscan)
kNNdistplot(gower_dist, k=19) +
  abline(h=0.16, col='red', lty=2)

res_col <- optics(gower_dist, eps=10, minPts = 7)

plot(res_col)
```

```{r}
#5 clusters, 21 noise points
res_col_d <- extractDBSCAN(res_col, eps_cl=0.0915)
plot(res_col_d)
```

```{r}
tsne_data_den_d<- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(res_col_d$cluster),
         name = data_clean$Country)

set.seed(5)
labels <- tsne_data_den_d %>% 
    filter(cluster != "0") %>% 
    group_by(cluster) %>%
    sample_n(.,1) 

tsne_data_den_d <- tsne_data_den_d %>% 
  mutate(label = ifelse(name %in% labels$name, as.character(name), ""))

ggplot(aes(x = X, y = Y), data = tsne_data_den_d) +
  geom_point(aes(color = cluster)) + 
  geom_text(aes(X,Y,label=label)) +
  scale_color_manual(values=c("#999999",
                              "#F8766D",
                              "#00BCD8",
                              "#E76BF3",
                              "#6BB100",
                              "#619CFF"))
```


#Predictive Tasks

##Decision Trees

```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(DMwR2)
library(rsample)
library(adabag)
library(ipred)
library(randomForest)
library(gbm)
library(performanceEstimation)
library(e1071)
library(nnet)
options(scipen = 999)
```

partition the data 
```{r}
data$Country = as.factor(data$Country)
data$Year = as.numeric(data$Year)


data$Life <- data$`Life expectancy`
data$infantmort <- cut(data$`infant deaths`, 
                              breaks = 3, 
                              labels=c('low_infant', 'middle_infant', 'high_infant'))
data$expenditure <- cut(data$`Total expenditure`, 
                              breaks = 3, 
                              labels=c('low_expenditure','med_expenditure', 'high_expenditure'))
data$pop <- cut(data$Population, 
                              breaks = 3, 
                              labels=c('low_pop','med_pop', 'high_pop'))
data$bmi <- cut(data$BMI, 
                              breaks = 3, 
                              labels=c('low_BMI','med_BMI', 'high_BMI'))  
data$gdp <- cut(data$GDP, 
                              breaks = 3, 
                              labels=c('low_GDP','med_GDP', 'high_GDP'))  
data$alcohol <- cut(data$Alcohol, 
                              breaks = 3, 
                              labels=c('low_alc','med_alc', 'high_alc'))  
summary(data$Year)
data$Year <- cut(data$Year, 
                              breaks = c(-Inf,2007, Inf),
                              labels=c("2000", "2007"))
summary(data$Year)
# data$Year <- cut(data$Year, breaks=c(-Inf, 2007, Inf), labels = c('2000-2007', '2007-2015'))  

drops <- c("Alcohol",
           "percentage expenditure",
           "BMI",
           "GDP",
           "Diphtheria",
           "Life expectancy",
           "Measles",
           "infant deaths",
           "Total expenditure", 
           "thinness  1-19 years",  
           "thinness 5-9 years", 
           "Population", 
           "HIV/AIDS", 
           "Polio",
           "under-five deaths",
           "Adult Mortality",
           "Income composition of resources",
           "Schooling",
           "Country")

data <- data[ , !(names(data) %in% drops)]
data <- data %>% drop_na

```

Decision Tree with one ensemble method
  Training and Testing Data
  Divide the data into 75% training, 25% test
```{r}
train_rows = sample(1:nrow(data), 0.75*nrow(data))
train = data[train_rows, ]
test = data[-train_rows, ]

```

Decision Tree (using regression):

The automatic cost complexity of the tree shows diminishing returns after 6 terminal nodes (which you can see in the cp, X-val Relative Error plot below the trees).

```{r}
m1 <- rpart(
  formula = Life ~ .,
  data = train,
 #cp = 0.01,
  method = "anova"
)

rpart.plot(m1)

```

Plot the tree that includes the observation number within each node:
```{r}
prp(m1, type = 1, extra = 101, roundint = FALSE)
```

#In the plot, y-axis is cross validation error, lower x-axis is cost complexity (α or cp) value, upper x-axis is the number of terminal nodes (tree size) 

We find diminishing returns after 6 terminal (leaf) nodes at a cost complexity parameter (cp) of .012.
You may also notice the dashed line which goes through the terminal node, tree size= 5.


```{r}
plotcp(m1)
```

This clustering tree has a control for cross validation X-val, and Cost complexity parameter (cp) where we now have a vertical line at 6, in relation to our leaf nodes in the decision tree.

The vertical reference line is drawn at the complexity parameter that has the lowest cross validated average squared error of the prediction, and the corresponding size for that complexity parameter is selected as the maximum leaf nodes of the tree.



To show the value of a 6 leaf node tree, we can see the plot below to review the diminishing returns as the tree continues to grow.

```{r}
m2 <- rpart(
   formula = Life ~ .,
   data    = train,
   method  = "anova", 
   control = list(cp = 0, xval = 10)
 )
 
plotcp(m2) + 
abline(v = 7, lty = "dashed")

#rpart.plot(m2)

```


Now we can review combinations of determining factors with the results from our regression trees by using different modeling examples.

We use a minimum spit of 5 and a maximum depth of 6 to match our splits and provide room for our terminal nodes.
```{r}
m3 <- rpart(
  formula = Life ~ .,
  data = train,
  method = "anova",
  control = list(minsplit = 5, maxdepth = 6, xval = 10)
)

m3$cptable
```

Then we search the grid, prividing room for our splits and leaf nodes in our ranges. The results show 91 different combinations, requiring 91 different models.
```{r}

hyper_grid <- expand.grid(
   minsplit = seq(3, 15, 1),
   maxdepth = seq(6, 12, 1)
 )
 head(hyper_grid)
 tail(hyper_grid)
 
 nrow(hyper_grid)
```


Now we can automate the modeling and save each model into its own item list.

```{r}
 models <- list()
 for (i in 1:nrow(hyper_grid)) {
   
   # get minsplit, maxdepth values at row i
   minsplit <- hyper_grid$minsplit[i]
   maxdepth <- hyper_grid$maxdepth[i]
   
   # train a model and store in the list
   models[[i]] <- rpart(
     formula = Life ~ .,
     data    = train,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
   )
 }

```

create a function to extract the minimum error associated with the optimal cost complexity value for each model. The error shows a .327 vs .328 and shows the maxdepth of diminishing leaf nodes does fall off after 6. 
```{r}

get_cp <- function(x) {
   min    <- which.min(x$cptable[, "xerror"])
   cp <- x$cptable[min, "CP"] 
 }

get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}
hyper_grid %>%
   mutate(cp    = purrr::map_dbl(models, get_cp),
          error = purrr::map_dbl(models, get_min_error)) %>%
   arrange(error) %>%
   top_n(-6, wt = error)
```

```{r}
optimal_tree <- rpart(
   formula = Life ~ .,
   data    = train,
   method  = "anova",
   control = list(minsplit = 10, maxdepth = 15, cp = 0.01)
 )
prp(optimal_tree, type=1, extra=101, roundint = FALSE)
 pred <- predict(optimal_tree, newdata = test)
 # pred
```


Variance between life expectancy for different models is 5.4
```{r}
  RMSE(pred = pred, obs = test$Life)
```


Some interesting rules from the decision tree
```{r}
rpart.rules(optimal_tree, cover = TRUE)
```


Bagging: learn trees on boostrapped samples using aggregated variables. Here we reduced error from 5.4 to 5.19.
```{r}
rfm <- ipred::bagging(Life ~ ., train, nbag=750)
rfpred <- predict(rfm, test)
RMSE(rfpred, test$Life)

```

Let's see if the Out-of-bag sample changes anything, OOB sample is 33% of th training data to estimate the model's accuracy and create a natural cross-validation process. Because the error is higher, we won't use the OOB sample.
```{r}
set.seed(123)

baga <- bagging(formula = Life ~., data = train, coob = TRUE)

baga

```

 The default bagging is 25 boostrap samples and trees, but we changed our metrics to review a broader scope with more samples. Here we can see that of the 100 tree samples used, the error stableizes when around 68 tree samples are used.
```{r}
ntree <- 10:100
rmse <- vector(mode = "numeric", length = length(ntree))
for (i in seq_along(ntree)) {
  set.seed(123)
  
  model <- bagging(
    formula = Life ~.,
    data = train,
    coob = TRUE,
    nbagg = ntree[i]
  )
  rmse[i] <-model$err
}

plot(ntree, rmse, type = "l", lwd = 2) +
abline(v=68, col = "blue", lty = "dashed")

```
We can compare this result to our hyper_grid results that show 91 models can be used to review decision tree combinations.


We can validate our results once more with CARET.
```{r}
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- train(
  Life ~ .,
  data = train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
  )
bagged_cv

```

We can look at the variables used in the models
```{r}
plot(varImp(bagged_cv), 13)  

```

Boosting:iteratively adds new models to the ensemble, each model tries to overcome the errors made by the previous model

```{r}

#train$Life <- as.factor(train$Life)
ct <- boosting(formula = Life ~ ., 
               data = train, 
               boos = TRUE,
               mfinal=10,
               control = (minsplit = 0))

ps2 <- predict(ct, test) 
ps2$confusion
ps2$error
```

#build model based on a forest of 750 trees and predict test data

```{r}
train$Life <- as.numeric(train$Life)
rfm <- randomForest(Life ~ ., train, ntree=750)

rfpred <- predict(rfm, test)

RMSE(rfpred, test$Life)


```


Another algorithm: Gradient boosting: classification and regression. like AdaBoost, but use gradient descent to address the current errors. Can overfit, so using cross-validation is very important (cv.folds)

```{r}
rfm <- gbm(Life ~ ., data=train, n.trees=150, cv.folds=3)
rfpred <- predict(rfm, test, n.trees=150)
RMSE(rfpred, test$Life)
```
