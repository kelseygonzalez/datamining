---
title: 'Final Project'
author: "Kelsey Gonzalez and Laura Werthmann"
date: "Due Dec 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE,fig.width=5)
```


Load packages
```{r}
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
library(performanceEstimation) # for performance estimation of svm
library(RCurl) # for performance estimation of svm
library(factoextra) #for hopkins statistic
library(clustertend) #for hopkins statistic
library(cluster) # for gower similarity, pam, and diana (a divisive hierarchical method). clara() is also included, which is basically pam by sampling
library(Rtsne) # for t-SNE plot
library(ggrepel)
library(dbscan)
library(rpart)
library(rpart.plot)
library(DMwR2)
library(rsample)
library(adabag)
library(ipred)
library(randomForest)
library(gbm)
library(performanceEstimation)
library(e1071)
library(nnet)
options(scipen = 999)

```

#Data Preprocessing

Review of World Health Organization's Life Expectancy Data taken from [Kaggle](https://www.kaggle.com/kumarajarshi/life-expectancy-who).

```{r}
rm(list=ls())
load("LifeExpectancyData.Rdata")
data$Status = as.factor(data$Status)
data = subset(data,select = -c(`Hepatitis B`))
```

Then we aquire data to include continent values
```{r}
# from https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes
ISO3166 <- read.csv(text = (getURL("https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv")))
ISO3166$name <- as.character(ISO3166$name)
# They use Eswatini instead of Swaziland, so let's update that & other naming mismatches
ISO3166$name[ISO3166$name == "Moldova, Republic of"] <- "Republic of Moldova"
ISO3166$name[ISO3166$name == "Korea, Republic of"] <- "Republic of Korea"
ISO3166$name[ISO3166$name == "Korea (Democratic People's Republic of)"] <- "Democratic People's Republic of Korea"
ISO3166$name[ISO3166$name == "Congo, Democratic Republic of the"] <- "Democratic Republic of the Congo"
ISO3166$name[ISO3166$name == "North Macedonia"] <- "The former Yugoslav republic of Macedonia"
ISO3166$name[ISO3166$name == "Tanzania, United Republic of"] <- "United Republic of Tanzania"
```

We created a country data frame to make the data join more easily, and then joined them by country("country", "name"), while adding the continent ("region")
```{r}
countries <- as.data.frame(unique(data$Country))
colnames(countries) <- "Country" 
countries$Country <- as.character(countries$Country)

dfcountry <- left_join(x = countries, 
                   y = ISO3166, 
                   by = c("Country" = "name"))


data <- left_join(x = data,
                   y = dfcountry[,c("Country","region")], 
                   by = c("Country" = "Country"))
```

```{r}
data$Status = as.factor(data$Status)
data$Country = as.factor(data$Country)

data <- data %>% select(-c(Polio, "percentage expenditure", Measles, `HIV/AIDS`)) 


dataavg <- data %>% 
  drop_na %>% 
  group_by(Country) %>% 
  summarize(Status = last(Status),
            Life = mean(`Life expectancy`),
            AdultMort = mean(`Adult Mortality`),
            infantdeath = mean(`infant deaths`),
            Alcohol = mean(Alcohol),
            BMI = mean(BMI),
            under_five_death = mean(`under-five deaths`),
            total_exp = mean(`Total expenditure`),
            Diptheria = mean(Diphtheria),
            GDP = mean(GDP),
            Pop = mean(Population),
            thinness_1_19  = mean(`thinness  1-19 years`),
            Income_comp = mean(`Income composition of resources`),
            Schooling = mean(Schooling),
            region = last(region)) 
summary(dataavg)

```


#Describe Data set


# not sure about these, just experimenting
```{r, echo=FALSE}
#http://rpubs.com/nicholas_dirienzo/523778
require(reshape2)

data_melted <- dataavg %>% 
  select(-Country, -Status, -region) %>% 
  scale() #make all the variables on the same scale

data_melted <- melt(as.data.frame(data_melted))

ggplot(data_melted,
       aes(x = value)) +
  geom_histogram() +
  scale_x_continuous(name="Distribution", limits=c(-2, 2)) +
  facet_wrap(~variable)
```

it looks like we need to log:
-infant deaths  
-under-fives deaths  
-GDP  
-Population  


```{r, echo=FALSE}
dataavg <- dataavg %>% mutate(log_infant_deaths = log(infantdeath+1) ,
                                log_under_five = log(under_five_death+1) ,
                                log_GDP = log(GDP),
                                log_Population = log(Pop)) %>% 
  select(-c(infantdeath,
            under_five_death,
            GDP,
            Pop))

data_melted <- dataavg %>% 
  select(-Country, -Status, -region) %>% 
  scale() #make all the variables on the same scale

data_melted <- melt(as.data.frame(data_melted))

ggplot(data_melted,
       aes(x = value)) +
  geom_histogram() +
  scale_x_continuous(name="Distribution", limits=c(-2, 2)) +
  facet_wrap(~variable)
```

```{r, message=FALSE}

set.seed(5)

ten_countries <- dataavg %>% 
  select(Country) %>% 
  sample_n(.,14) 
```

```{r, echo=FALSE, message = FALSE}
dataavg %>% 
  mutate(label = ifelse(Country %in% ten_countries$Country, 
                        as.character(Country), 
                        "")) %>% 
  ggplot(aes(BMI, Life)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2") + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "BMI",
       y = "life expectancy") +
  geom_smooth()
```

```{r, echo=FALSE, message = FALSE}
dataavg %>% 
  mutate(label = ifelse(Country %in% ten_countries$Country, 
                        as.character(Country), 
                        "")) %>% 
  ggplot(aes(log_GDP, Life)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2") + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "logGDP",
       y = "life expectancy")+
  geom_smooth()
```

```{r, echo=FALSE, message = FALSE}
dataavg %>% 
  mutate(label = ifelse(Country %in% ten_countries$Country, 
                        as.character(Country), 
                        "")) %>% 
  ggplot(aes(log_infant_deaths, Life)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2") + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "Infant Deaths (logged)",
       y = "life expectancy")+
  geom_smooth()
```

```{r, echo=FALSE, message = FALSE}
dataavg %>% 
  mutate(label = ifelse(Country %in% ten_countries$Country, 
                        as.character(Country), 
                        "")) %>% 
  ggplot(aes(Diptheria, Life)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2") + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "Diptheria",
       y = "life expectancy")+
  geom_smooth()
```

```{r, echo=FALSE, message = FALSE}
dataavg %>% 
  mutate(label = ifelse(Country %in% ten_countries$Country, 
                        as.character(Country), 
                        "")) %>% 
  ggplot(aes(log_Population, Life)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2") + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "Population (Logged)",
       y = "life expectancy")+
  geom_smooth()


```


```{r, echo=FALSE, message = FALSE}
dataavg %>% 
  mutate(label = ifelse(Country %in% ten_countries$Country, 
                        as.character(Country), 
                        "")) %>% 
  ggplot(aes(AdultMort, Life)) +
  geom_point(size = 3.5, 
             alpha = .9,
             shape = 21, 
             col = "white", 
             fill = "#0162B2") + 
  geom_text_repel(aes(label = label),
                  size = 4.5,
                  point.padding = .2,
                  box.padding = .3,
                  force = 1,
                  min.segment.length = 0) +
  theme_minimal(14) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank()) +
  labs(x = "Adult Mortality",
       y = "life expectancy")+
  geom_smooth()

```







#Descriptive Analyses
## Pam Clustering 


First things first, do our data even have a natural clustering tendency?
the larger the result, the higher clustering tendency

The hopkins statistic on our dataset shows about a .79 clustering tendency, where in some cases can resemble 90% confidence level.
source: https://www.datanovia.com/en/lessons/assessing-clustering-tendency/
Compute Hopkins statistic for dataset:n must be no larger than num of samples

```{r}
# Hopkins, for numerical data only 

data_no_cat <- dataavg %>% 
  select(-Country, -Status, -region) %>% 
  as.matrix()
rownames(data_no_cat) <- dataavg$Country

res <- get_clust_tendency(data_no_cat, n=139, graph = TRUE)
res$hopkins_stat

```

```{r}
res$plot
```


Our Next try shows the clustering tendency at .25 which is very small; < 0.5, no clustering tendency
```{r}
set.seed(78)
res <- hopkins(data_no_cat, n = 137)
res$H 
```

Calculate distance between attributes using gower.
```{r}
gower_dist <- daisy(dataavg[, -1], 
                    metric = "gower")


# Check attributes to ensure the correct methods are being used (I = interval, N = nominal)
# Note that despite logratio being called, the type remains coded as "I"

# cleaning it up
gower_mat <- as.matrix(gower_dist)
rownames(gower_mat) <- colnames(gower_mat) <- dataavg$Country

```

Calculate Silhouette width for 2 to 10 clusters using PAM - The top cluster options are 2 and 3
```{r}
sil <- c(NA)
for(i in 2:10){
  pam_fit <- pam(gower_mat, diss=TRUE, k=i)
  sil[i] <-pam_fit$silinfo$avg.width
}

plot(1:10, sil,
xlab = "Number of clusters",
ylab = "Silhouette Width",
lines(1:10, sil))
```

review 2 clusters
```{r}
pam_fit <- pam(gower_dist, diss = TRUE, k = 2)

# add cluster labels to the data. We will use result1 later
data_clean <- data.frame(dataavg, pam_fit$cluster)

# show clustering results by country
result1 <- data_clean %>% select(Country,pam_fit.cluster)
clusplot(pam_fit)

```

Review 3 clusters
```{r}
pam_fit <- pam(gower_dist, diss = TRUE, k = 3)

# add cluster labels to the data. We will use result1 later
data_clean <- data.frame(dataavg, pam_fit$cluster)

# show clustering results by country
result1 <- data_clean %>% select(Country,pam_fit.cluster)
clusplot(pam_fit)
```


group_by cluster and then compute the summary data (means, median, etc) for each cluster
```{r}
data_results <- data_clean %>%
  mutate(cluster = pam_fit.cluster) %>% #add the cluster column
  select(-pam_fit.cluster) %>% 
  group_by(cluster) %>% #group countries by its cluster 
  do(the_summary = summary(.)) #do: summarize by group/cluster,add the_summary column

data_results$the_summary
```
The results suggest:

Cluster 1 has all developing countries (53) with a median Life Expectancy of around 58 years, and is mainly comprised of 38 countries in Africa, 13 in Asian, 1 in the Americas, and 1 in Oceana.

Cluster 2 has all developing countries (55) with a median Life Expectancy of around 73 years, and is mainly comprised of 22 countries in the Americas, 17 in Asia, 6 in Oceana, 5 in Africa, and 5 in Europe.

Cluster 3 has mostly developed countries (26) and 6 developing countries with a median Life Expectancy of around 80 years, and is mainly comprised of countries in Europe, 29, and Oceania, 1.




One way to visualize many variables in a lower dimensional space is with t-distributed stochastic neighborhood embedding, or t-SNE. 
```{r}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)

tsne_data_pam <- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering),
         name = data_clean$Country)

ggplot(aes(x = X, y = Y), data = tsne_data_pam) +
  geom_point(aes(color = cluster)) + 
  geom_text(data=subset(tsne_data_pam[pam_fit$medoids, ]), # add names of centroids in
            aes(X,Y,label=name))
```
clearly pam isn't the ideal clustering algorithm here. 

## Hierarchical Clustering 

We can use hierarchical clustering to show where the best fit clusters might be.
```{r}
h <- hclust(d = as.dist(gower_dist), method="complete")
plot(h, cex = 0.3, hang = -1, main="Hierarchal Cluster of Predicted Probabilities") +
  abline(h=quantile(gower_dist, .75), col="blue")
```

```{r}
tsne_data_hc <- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(cutree(h, h=quantile(gower_dist, .75))),
         name = data_clean$Country)

set.seed(8)
labels <- tsne_data_hc %>%
    group_by(cluster) %>%
    sample_n(.,1) 

tsne_data_hc <- tsne_data_hc %>% 
  mutate(label = ifelse(name %in% labels$name, as.character(name), ""))

ggplot(aes(x = X, y = Y), data = tsne_data_hc) +
  geom_point(aes(color = cluster)) + 
  geom_text(aes(X,Y,label=label))

```

## Density Clustering with Optics


```{r}
#16 dimensions, so set k = 17

kNNdistplot(gower_dist, k=17) +
  abline(h=0.2, col='red', lty=2)

res_col <- optics(gower_dist, eps=10, minPts = 7)

plot(res_col)
```

Density clustering with DBSCAN
```{r}
#4 clusters
res_col_d <- extractDBSCAN(res_col, eps_cl=0.117)
plot(res_col_d)
```

```{r}
tsne_data_den_d<- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(res_col_d$cluster),
         name = data_clean$Country)

set.seed(5)
labels <- tsne_data_den_d %>% 
    filter(cluster != "0") %>% 
    group_by(cluster) %>%
    sample_n(.,1) 

tsne_data_den_d <- tsne_data_den_d %>% 
  mutate(label = ifelse(name %in% labels$name, as.character(name), ""))

ggplot(aes(x = X, y = Y), data = tsne_data_den_d) +
  geom_point(aes(color = cluster)) + 
  geom_text(aes(X,Y,label=label)) +
  scale_color_manual(values=c("#999999",
                              "#F8766D",
                              "#00BCD8",
                              "#E76BF3",
                              "#6BB100",
                              "#619CFF"))
```

The values are all over the place, making sense why these are "outliers" according to optical clustering. 

30 countries are outliers, but which?

```{r}
tsne_data_den_d %>% 
    filter(cluster == "0") %>% 
    select(name) 


dataavg %>% filter(Country %in% as.vector(tsne_data_den_d$name[tsne_data_den_d$cluster == 0])) %>% summary()
# the values are all over the place, making sense why these are "outliers" according to optical clustering. 

```

#Predictive Tasks

##Decision Trees
Decision Tree with one ensemble method
  Training and Testing Data
  Divide the data into 75% training, 25% test
```{r}
dataavg <- dataavg %>% select(-Country)

train_rows = sample(1:nrow(dataavg), 0.75*nrow(dataavg))
train = dataavg[train_rows, ]
test = dataavg[-train_rows, ]

```

Decision Tree (using regression):

The automatic cost complexity of the tree shows diminishing returns after 6 terminal (leaf) nodes.
```{r}

m1 <- rpart(
  formula = Life ~ .,
  data = train,
  cp = 0.01,
  method = "anova"
)
#prp(m1, type = 1, extra = 101, roundint = FALSE)

library(RColorBrewer)
library(rattle)
fancyRpartPlot(m1)

```


#In the plot, y-axis is cross validation error, lower x-axis is cost complexity (α or cp) value, upper x-axis is the number of terminal nodes (tree size) 

We find diminishing returns after 6 terminal (leaf) nodes at a cost complexity parameter (cp).
You may also notice the dashed line which goes through the terminal node, tree size= 4.
The vertical reference line is drawn at the complexity parameter that has the lowest cross validated average squared error of the prediction, and the corresponding size for that complexity parameter is selected as the maximum nodes of the tree.

```{r}
plotcp(m1, col = "blue") + 
abline(v = 4, col = "red", lty = "dashed")
```


This tree has a control for cross validation X-val, and Cost complexity parameter (cp) where we now have a vertical line at 5, in relation to our leaf nodes in the decision tree. Shown below.

The vertical reference line is drawn at the complexity parameter that has the lowest cross validated average squared error of the prediction, and the corresponding size for that complexity parameter is selected as the maximum nodes of the tree.

To show the value of a 6 maximum node tree, we can see the plot below to review the diminishing returns as the tree continues to grow.

```{r}
m2 <- rpart(
   formula = Life ~ .,
   data    = train,
   method  = "anova", 
   control = list(cp = 0, xval = 10)
 )
 
plotcp(m2, col = "blue") + 
abline(v = 5, col = "green", lty = "dashed")

fancyRpartPlot(m2, col = "White")

```


Now we can review combinations of determining factors with the results from our regression trees by using different modeling examples.
We use a minimum split of 4 and a maximum depth of 6 to match our splits and provide room for our terminal nodes.
```{r}
m3 <- rpart(
  formula = Life ~ .,
  data = train,
  method = "anova",
  control = list(minsplit = 0, maxdepth = 15, xval = 10)
)

m3$cptable
rpart.plot(m3)

```

Then we search the grid, prividing room for our splits and leaf nodes in our ranges. The results show 91 different combinations, requiring 91 different models.
```{r}

hyper_grid <- expand.grid(
   minsplit = seq(3, 15, 1),
   maxdepth = seq(6, 12, 1)
 )
 head(hyper_grid)
 tail(hyper_grid)
 
 nrow(hyper_grid)
```


Now we can automate the modeling and save each model into its own item list.
```{r}
 models <- list()
 for (i in 1:nrow(hyper_grid)) {
   
   # get minsplit, maxdepth values at row i
   minsplit <- hyper_grid$minsplit[i]
   maxdepth <- hyper_grid$maxdepth[i]
   
   # train a model and store in the list
   models[[i]] <- rpart(
     formula = Life ~ .,
     data    = train,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
   )
 }

```

#Review Description!!

create a function to extract the minimum error associated with the optimal cost complexity value for each model. The error shows a .126 vs .134.
```{r}

get_cp <- function(x) {
   min    <- which.min(x$cptable[, "xerror"])
   cp <- x$cptable[min, "CP"] 
 }

get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}
hyper_grid %>%
   mutate(cp    = purrr::map_dbl(models, get_cp),
          error = purrr::map_dbl(models, get_min_error)) %>%
   arrange(error) %>%
   top_n(-6, wt = error)
```


Now we can view the variance of LIfe Expectancy for different models is 3.51

```{r}
optimal_tree <- rpart(
   formula = Life ~ .,
   data    = train,
   method  = "anova",
   control = list(minsplit = 3, maxdepth = 15, cp = 0.01)
 )
prp(optimal_tree, type=1, extra=101, roundint = FALSE)
 pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Life)
```


Some interesting rules from the decision tree
```{r}
rpart.rules(optimal_tree, cover = TRUE)
```


Bagging: learn trees on boostrapped samples using aggregated variables. Here we reduced error from 3.51 to 3.06.
```{r}
rfm <- ipred::bagging(Life ~ ., train, nbag=150)
rfpred <- predict(rfm, test)
RMSE(rfpred, test$Life)

```

Let's see if the Out-of-bag (OOB) sample changes anything. The OOB sample uses 33% of the training data to estimate the model's accuracy and create a natural cross-validation process. Because the error is higher (3.46) than our original bagging, we won't use the OOB sample.
```{r}
set.seed(123)

baga <- bagging(formula = Life ~., data = train, coob = TRUE)

baga

```

The default bagging is 25 boostrap samples and trees, but we changed our metrics to review a broader scope with more samples. Here we can see that of the 150 tree samples used, the error more or less stableizes after around 30 tree samples are used, then has a slight rise and just falls after that.
```{r}
ntree <- 10:100
rmse <- vector(mode = "numeric", length = length(ntree))
for (i in seq_along(ntree)) {
  set.seed(123)
  
  model <- bagging(
    formula = Life ~.,
    data = train,
    coob = TRUE,
    nbagg = ntree[i]
  )
  rmse[i] <-model$err
}

min(rmse)
plot(ntree, rmse, type = "l", lwd = 2) +
abline(v=30, col = "blue", lty = "dashed") +
  abline(v=116, col = "green", lty = "dashed")
```

```{r}
#suppressPackageStartupMessages(install.packages("inTrees"))
#library(inTrees)
#extractRules(rmse, X, ntree = 100, maxdepth = 6, random = FALSE, digits = NULL)

```


We can validate our results once more with CARET. Using random sub-samples of our dataset we get a mean squared error of 3.16 still a little higher than the original bagging sample, but still low.
```{r}
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- train(
  Life ~ .,
  data = train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
  )
bagged_cv



```

We can look at the variables used in the models to predict their importance and get a random mean squared error of 3.059, using the CARET method, the lowest yet.
```{r}
plot(varImp(bagged_cv), 15)

predct <- predict(bagged_cv, test)
RMSE(predct, test$Life)

```

Boosting:iteratively adds new models to the ensemble, each model tries to overcome the errors made by the previous model

```{r}
#ct <- boosting(formula = Life ~ ., 
 #              data = train, ntree=150,
  #             boos = TRUE,
   #            mfinal=10,
    #           control = rpart.control(cp = 1))

#ct1 <- boosting(formula = Life ~ ., 
 #              data = train, 
  #             boos = TRUE,
   #            mfinal=10,
    #           control = rpart.control(minsplit = 0))

#ps2 <- predict(ct, test) 
#ps2$confusion
#ps2$error
```

Random Forest: build model based on a forest of 750 trees and predict test data

```{r}
train$Life <- as.numeric(train$Life)
rfm <- randomForest(Life ~ ., train, ntree=750)

rfpred <- predict(rfm, test)

RMSE(rfpred, test$Life)


```

```{r}
suppressPackageStartupMessages(install.packages("party"))
library(randomForest)
cforest <- randomForest(Life ~ ., data=test, controls=cforest_control(mtry=2, mincriterion=0))
getTree(cforest, 1, labelVar = TRUE)
```


Gradient boosting uses classification and regression. like AdaBoost, but use gradient descent to address the current errors. The method can overfit, so using cross-validation is very important (cv.folds). The default method uses gaussian.

```{r}
rfm <- gbm(Life ~ .,
           data=train,
           n.trees=150,
           cv.folds=10)

rfpred <- predict(rfm, 
                  test,
                  n.trees=150)

RMSE(rfpred, test$Life)

```
