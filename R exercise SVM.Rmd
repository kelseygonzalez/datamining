---
title: 'R extra exercise: Support Vector Machines'
author: "Kelsey Gonzalez and Laura Werthmann"
date: "Extra Credit"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE,fig.width=5)
```


Load packages
```{r}
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots
library(performanceEstimation) # for performance estimation of svm
```

set pseudorandom number generator
```{r}
set.seed(10)
```
Construct sample data set - completely separated
randomly generate 20 observations, one observation has two attributes
```{r}
x <- matrix(rnorm(20*2), ncol = 2)
#created class labels: -1 and 1. They will be treated as factors, so any two different labels would do, for example, 0 vs. 1
y <- c(rep(-1,10), rep(1,10))
#add 3/2 to the values in the 'y=1' class, so now the instances in the two classes are separated
x[y==1,] <- x[y==1,] + 3/2
#combine x and factor y to make the dataframe for the classifier.
dat <- data.frame(x=x, y=as.factor(y))
```
Plot data to see one can draw a line btw the two classes to seperate them
```{r}
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
    geom_point(size = 2) +
    scale_color_manual(values=c("#000000", "#FF0000")) +
    theme(legend.position = "none")
```

Fit Support Vector Machine model to data set
```{r}
svmfit <- svm(y~., data = dat, kernel = "linear")
```
Plot Results
```{r}
plot(svmfit, dat)
```
In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line.

same result can be obtained using kernlab package
fit model and produce plot
```{r}
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'vanilladot')
plot(kernfit, data = x)
```
In this plat, a color gradient that indicates how confidently a new point would be classified based on its features.
 

Next We simulate a new data set where the classes are more mixed.
```{r}
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))
#Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
    geom_point(size = 2) +
    scale_color_manual(values=c("#000000", "#FF0000")) +
    theme(legend.position = "none")
```

Fit Support Vector Machine model to data set using svm
```{r}
svmfit <- svm(y~., data = dat, kernel = "linear")
#Plot Results
plot(svmfit, dat)
```
We can see data points that are mis-classfied (both sides has 0s and Xs)
 
We can adjust the cost parameter to influnce where the boundary is drawn
use tune() to test varous costs and find the best fitting model
find optimal cost of misclassification
```{r}
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
                   ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)
```
Create a table of misclassified observations
```{r}
ypred <- predict(bestmod, dat)
(misclass <- table(predict = ypred, truth = dat$y))
```


Work further to use svm() to classify nonlinear data
```{r}
set.seed(123)
#construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
#first 100 data points are moved up by adding 2.5
x[1:100,] <- x[1:100,] + 2.5
#another 50 data points are moved down by taking away 2.5
x[101:150,] <- x[101:150,] - 2.5
```
the last 50 didn't change their values
this creates a data "sandwich" as shown later in the scattered plot.
you can't draw a straight line to separate class 1 from class 2

we are now labeling the classes as 1 and 2.
```{r}
y <- c(rep(1,150), rep(2,50)) 
dat <- data.frame(x=x,y=as.factor(y))
#Plot data

ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
    geom_point(size = 2) +
    scale_color_manual(values=c("#000000", "#FF0000")) +
    theme(legend.position = "none")
```
we’ll take 100 random observations from the set as training example and use them to  construct our boundary. We set kernel = "radial" based on the shape of our data and plot the results.

set pseudorandom number generator
```{r}
set.seed(123)
```
sample training data and fit model
generate 100 random indices to select training data
```{r}
train <- base::sample(200,100, replace = FALSE)
svmfit <- svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
#plot classifier: looks good
plot(svmfit, dat[train,])
``` 
try other kernels, they don't work as well as the radial (which is good for "sandwich" like data)
```{r}
#linear kernel
svmfit <- svm(y~., data = dat[train,], kernel = "linear", gamma = 1, cost = 1)
plot(svmfit, dat[train,]) 
#polynomial kernel
svmfit <- svm(y~., data = dat[train,], kernel = "polynomial", gamma = 1, cost = 1)
plot(svmfit, dat[train,]) 
#sigmoid kernel
svmfit <- svm(y~., data = dat[train,], kernel = "sigmoid", gamma = 1, cost = 1)
plot(svmfit, dat[train,])
```

Fit radial-based SVM in kernlab. ksvm has more kernels to choose from
```{r}
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'rbfdot', C = 1, scaled = c())
#Plot training data
plot(kernfit, data = x[train,])

#try other kernels, most bad except for lapacedot
#polydot kernel
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'polydot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
#vanilladot kernel
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'vanilladot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
#tanhdot kernel
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'tanhdot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
#laplacedot kernel, looks good
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'laplacedot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
#anovadot kernel
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'anovadot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
#splinedot kernel
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'splinedot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
#besseldot kernel
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'besseldot', C = 1, scaled = c())
plot(kernfit, data = x[train,])
```

can we tune it and make it better?
```{r}
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                               gamma=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
#extract the best model
(bestmod <- tune.out$best.model)
plot(bestmod, dat[train, ])
```

validate model performance on test data
```{r}
ypred <- predict(bestmod, dat[-train, ])
(valid <- table(true = dat[-train,"y"], pred = ypred))
```
mutiple class classification using SVM is very similar to the binary classification examples shown above. The Khan data set contains data on 83 tissue samples with 2308 gene expression measurements on each sample. These were split into 63 training observations and 20 testing observations, and there are four distinct classes in the set. It would be impossible to visualize such data, so we start with the simplest classifier (linear) to construct our model. We will use the svm command from e1071 to conduct our analysis.

fit model (you can always use tune() to find the best parameter)
```{r}
dat <- data.frame(x = Khan$xtrain, y=as.factor(Khan$ytrain))
(out <- svm(y~., data = dat, kernel = "linear", cost=10))
```

check model performance on training set: 100% precision and recall
```{r}
table(out$fitted, dat$y)
```

validate model performance
```{r}
dat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te <- predict(out, newdata=dat.te)
table(pred.te, dat.te$y) 
```  
classified 2 instances wrong -- overall a pretty good performance



# [REQUIRED]
##1.	Try out SVM on your dataset and report performances using two different sets of parameters.
```{r}
rm(list=ls())
load("LifeExpectancyData_3.Rdata")
data$Status = as.factor(data$Status)
data$Year = as.factor(data$Year)
data$Country = as.factor(data$Country)

data <- data %>% filter(Year=="2013") %>% select(-c(Year, Country) %>% drop_na()

samples <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[samples, ]
test <- data[-samples,]
```

### construct a basic SVM model
```{r}
model <- svm(Status~., 
             data = train, 
             kernel = "linear", 
             type="C-classification", 
             cost=1, 
             na.action = na.omit)
# plot(model, data[samples,]) can't get this to work. 

```

```{r}
score <- function(model, test){
    pred <- predict(model, test, na.action=na.omit)
    t <- table(pred, test$Status)
    return (score = sum(diag(t))/sum(t))
}
```

```{r}
score(model, test) #0.60
score(model, train) #greater the cost, more fit the model is to the train (low bias, high variance -- risk overfitting)
```


```{r}
summary(model)
```


###kernels
run 5-fold cross validation
```{r}
model <- svm(Status~., data = data, kernel = "linear", type="C-classification", na.action = na.omit, cross=5)
model$accuracies
```

```{r}
set.seed(1)
radial <- svm(Status~., data = data, kernel = "radial",type="C-classification", gamma = 0.1, cost = 1, cross=5)
radial$accuracies
```
smaller gamma: decision boundary not very curvy or pointy,  higher bias and lower variance, risk underfitting
higher gamma: lower bias and high variance, risk overfitting
smaller C: allow more errors. high bias, low variance, underfitting
larger C: allow few errors. low bias, high variance, overfitting

###tuning: estimate performances using different combination of parameters:grid search
```{r}
set.seed(1)
tune.out <- tune(svm, 
                 Status~., 
                 data = data, 
                 kernel = "polynomial", 
                 type="C-classification", 
                 na.action=na.omit,
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5), 
                               gamma = c(0.001, 0.01, 0.1, 1, 5), 
                               degree = c(2, 3, 4)))
tune.out$best.parameters
tune.out$best.performance #error rate, the lower the better
```
_It looks like the best performance is with a cost of 1, gamma of 0.1 and 3 degrees. This yields an error rate of 0.064._

use error.fun in parameter tunecontrol= to specify performance metrics to use
```{r}
set.seed(1)
tune.out <- tune(svm, Status~., 
                 data = data[samples, ],
                 kernel = "linear", 
                 type="C-classification",
                 na.action=na.omit,
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1)))
tune.out
```
extract the best model
```{r}
(bestmod <- tune.out$best.model)
```

## 2.	Use Performance Estimates learned last week to find a set of best parameters for your SVM. 

### performanceEstimation
```{r}
set.seed(1)
res <- performanceEstimation(
  c(PredTask(Status ~ .,data)),
  c(workflowVariants(learner="svm",
                     learner.pars=list(cost=c(0.01, 1), kernel=c("linear", "radial")))),
  EstimationTask(metrics="acc", method=CV(nReps=1, nFolds=3))
)
```

```{r}
summary(res)
```

```{r}
res$data.Status
```
	
