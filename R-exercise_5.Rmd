---
title: 'R exercise 5: Classification'
author: "Kelsey Gonzalez and Laura Werthmann"
date: "Due Nov 6, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE,fig.width=5)
```

First we'll download the necessary packages, libraries and data.
```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(DMwR2)
library(rsample)
library(adabag)
library(ipred)
library(randomForest)
library(gbm)
library(performanceEstimation)
library(e1071)
library(nnet)


options(scipen = 999)
```


```{r echo=FALSE}
rm(list=ls())
load("LifeExpectancyData_3.Rdata")
data$Status = as.factor(data$Status)
#data$Year = as.factor(data$Year)
data$Country = as.factor(data$Country)


# data$Life <- cut(data$`Life expectancy`, 
#                               breaks = 3, 
#                               labels=c('low LE', 'middle LE', 'high LE'))

data$Life <- data$`Life expectancy`

data$infantmort <- cut(data$`infant deaths`, 
                              breaks = 3, 
                              labels=c('low_infant', 'middle_infant', 'high_infant'))
data$expenditure <- cut(data$`Total expenditure`, 
                              breaks = 3, 
                              labels=c('low_expenditure','med_expenditure', 'high_expenditure'))
data$pop <- cut(data$Population, 
                              breaks = 3, 
                              labels=c('low_pop','med_pop', 'high_pop'))
data$bmi <- cut(data$BMI, 
                              breaks = 3, 
                              labels=c('low_BMI','med_BMI', 'high_BMI'))  
data$gdp <- cut(data$GDP, 
                              breaks = 3, 
                              labels=c('low_GDP','med_GDP', 'high_GDP'))  
data$alcohol <- cut(data$Alcohol, 
                              breaks = 3, 
                              labels=c('low_alc','med_alc', 'high_alc'))  
<<<<<<< HEAD
summary(data$Year)

data$Year <- cut(data$Year, 
                              breaks = c(-Inf,2007, Inf),
                              labels=c("2000", "2007"))
summary(data$Year)
=======
# data$Year <- cut(data$Year, breaks=c(-Inf, 2007, Inf), labels = c('2000-2007', '2007-2015'))  

<<<<<<< HEAD



>>>>>>> db583c2569f2251570ca4fbe20847f1bbfc1fa5a

=======
>>>>>>> efab0a881a09876dc10084255a352dcb2e6130fb
drops <- c("Alcohol",
           "percentage expenditure",
           "BMI",
           "GDP",
           "Diphtheria",
           "Life expectancy",
           "Measles",
           "infant deaths",
           "Total expenditure", 
           "thinness  1-19 years",  
           "thinness 5-9 years", 
           "Population", 
           "HIV/AIDS", 
           "Polio",
           "under-five deaths",
           "Adult Mortality",
           "Income composition of resources",
           "Schooling",
           "Country")
data <- data[ , !(names(data) %in% drops)]
<<<<<<< HEAD
data <- data %>% drop_na
=======
data <- data %>% drop_na()

>>>>>>> db583c2569f2251570ca4fbe20847f1bbfc1fa5a

# write.csv(data, file = "LifeExpectancyData_all_cat.Rdata")
```
____
<<<<<<< HEAD
#[REQUIRED]
Exercises on your data set:   
=======
#[REQUIRED] 

Exercises on your data set:  

>>>>>>> db583c2569f2251570ca4fbe20847f1bbfc1fa5a
## 1.	Build a decision tree (regression or classification) and use one ensemble method on your own dataset.

### traing and test data
First, divide the data into 75% data for training, and 25% for test
```{r}
train_rows = sample(1:nrow(data), 0.75*nrow(data))
train = data[train_rows, ]
test = data[-train_rows, ]
<<<<<<< HEAD

```


=======
```

>>>>>>> db583c2569f2251570ca4fbe20847f1bbfc1fa5a
#### build a classification tree

```{r}
m1 <- rpart(
  formula = Life ~ ., 
  data    = train,
  cp   = 0.01,
  method  = "anova"
  )
```

#### plot the classification trees
```{r}

prp(m1, type=1, extra=101, roundint = FALSE)
```


In the plot, y-axis is cross validation error, lower x-axis is cost complexity (Î± or cp) value, upper x-axis is the number of terminal nodes (tree size) 
We find diminishing returns after 6 terminal nodes.
You may also notice the dashed line which goes through the point tree size= 6.
```{r}
plotcp(m1)

```


```{r}
m2 <- rpart(
   formula = Life ~ .,
   data    = train,
   method  = "anova", 
   control = list(cp = 0, xval = 10)
 )
 
plotcp(m2)
abline(v = 6, lty = "dashed") 
#vertical line at tree size = 6.

```

#### tune the model
try out different combinations of minsplit and maxdepth values and compare the models

Testing out different models
```{r}
hyper_grid <- expand.grid(
   minsplit = seq(5, 20, 1),
   maxdepth = seq(8, 15, 1)
 )
 # head(hyper_grid)
 
 
 models <- list()
 for (i in 1:nrow(hyper_grid)) {
   
   # get minsplit, maxdepth values at row i
   minsplit <- hyper_grid$minsplit[i]
   maxdepth <- hyper_grid$maxdepth[i]
   
   # train a model and store in the list
   models[[i]] <- rpart(
     formula = Life ~ .,
     data    = train,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
   )
}


# function to get optimal cp
get_cp <- function(x) {
   min    <- which.min(x$cptable[, "xerror"])
   cp <- x$cptable[min, "CP"] 
 }


# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}

hyper_grid %>%
   mutate(cp    = purrr::map_dbl(models, get_cp),
          error = purrr::map_dbl(models, get_min_error)) %>%
   arrange(error) %>%
   top_n(-5, wt = error)



```


```{r}
optimal_tree <- rpart(
   formula = Life ~ .,
   data    = train,
   method  = "anova",
   control = list(minsplit = 20, maxdepth = 11, cp = 0.01)
 )
prp(optimal_tree, type=1, extra=101, roundint = FALSE)
```

```{r}
pred <- predict(optimal_tree, newdata = test)
RMSE(pred = pred, obs = test$Life)
#average error is about 5.48
```
____


##2. List at least two interesting rules from the decision tree your generated. 
```{r}
rpart.rules(optimal_tree, cover = TRUE)
```

1. Americas or Europe with high BMI has relatively high life expectancy  
2. African countries with low BMI have the shortest expected life span   
  


____
## 3. Does the ensemble method improve the performance on your dataset?
bagging: learn trees on boostrapped samples using all variables
```{r}
# 750 trees built to bootstrapped samples using all variables
rfm <- ipred::bagging(Life ~ ., train, nbag=750)
rfpred <- predict(rfm, test)
#error reduced from 39145 to 34536
RMSE(rfpred, test$Life)

```
earier RMSE was 5.48323, so 5.335 with bagging is an improvement. 


random forest is a robust approach to improve tree prediction performance
```{r}

#build model based on a forest of 750 trees
rfm <- randomForest(Life ~ ., train, ntree=750)

#predict test data 
rfpred <- predict(rfm, test)

#error reduced from 39145.39 to 24226.48.
RMSE(rfpred, test$Life)

```

5.48323 < 5.335 < 5.094837
Random Forest gibes us the best results (lowest RMSE)  

