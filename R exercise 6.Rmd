---
title: 'R exercise 6: Paritioning and Hierarchal Clustering'
author: "Kelsey Gonzalez and Laura Werthmann"
date: "Due Nov 27, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r global_options, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE,fig.width=5)
```


Load packages
```{r}
library(tidyverse) # for data cleaning and visualization
library(ISLR) # for college dataset
library(cluster) # for gower similarity, pam, and diana (a divisive hierarchical method). clara() is also included, which is basically pam by sampling
library(Rtsne) # for t-SNE plot
library(ggdendro)
library(factoextra) #for hopkins statistic
library(clustertend) #for hopkins statistic


```

Clustering requires 
1. Calculating distance among the observations
2. Choosing a clustering algorithm
3. Selecting the number of clusters

load data 
```{r}
rm(list=ls())
load("LifeExpectancyData_3.Rdata")
data$Status = as.factor(data$Status)
data$Year = as.factor(data$Year)
data$Country = as.factor(data$Country)

data <- data %>% filter(Year=="2013") %>% select(-c(Year, Polio, "percentage expenditure", Measles, `HIV/AIDS`)) %>% drop_na()

#create a quick df with only numberical variables
data_no_cat <- data %>% select(-Status, -region, -Country)
data_no_cat <- as.data.frame(data_no_cat)
rownames(data_no_cat) <- data$Country
```

First things first, do our data even have a natural clustering tendency?
the smaller the result, the higher clustering tendency
```{r}
#Hopkins, for numerical data only 
# Compute Hopkins statistic for iris dataset:n must be no larger than num of samples
# seed set internally in the function, always give the same result.
res <- get_clust_tendency(data_no_cat, n=138, graph = TRUE)
res$hopkins_stat
res$plot

set.seed(1)
res <- hopkins(data_no_cat, n = 138)
res$H #very small, < 0.5, good clustering tendency
```



calculating distance the Gower distance for mixed variable types in package cluster

**_Gower distance_:** For each variable type, a particular distance metric that works well for that type is used and scaled to fall between 0 and 1. Then, a linear combination using user-specified weights (most simply an average) is calculated to create the final distance matrix.   

The metrics used for each data type are described below:  

_quantitative (interval)_: range-normalized Manhattan distance  
_ordinal_: variable is first ranked, then Manhattan distance is used with a special adjustment for ties  
_nominal_: variables of k categories are first converted into k binary columns and then the Dice coefficient is used  

_pros_: Intuitive to understand and straightforward to calculate 

_cons_: Sensitive to non-normality and outliers present in continuous variables, so transformations as a pre-processing step might be necessary. Also requires an NxN distance matrix to be calculated, which is computationally intensive to keep in-memory for large samples
Note that due to positive skew in the Enroll variable, a log transformation is conducted internally via the type argument. Instructions to perform additional transformations, like for factors that could be considered as asymmetric binary (such as rare events), can be seen in ?daisy.

Remove college name before clustering
Use daisy() to compute the distance matrix using ‘gower’

```{r}
gower_dist <- daisy(data[, -1],
                    metric = "gower",
                    type = list(logratio = "GDP", 
                                logratio = "Population"))

# Check attributes to ensure the correct methods are being used (I = interval, N = nominal)
# Note that despite logratio being called, the type remains coded as "I"
summary(gower_dist)

# cleaning it up
gower_mat <- as.matrix(gower_dist)
rownames(gower_mat) <- colnames(gower_mat) <- data$Country


```


Output most similar pair, excluding dissimilar scores of 0 (one compared to itself)
```{r}
data[
  which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]),
        arr.ind = TRUE)[1, ], ]

data[which(gower_mat==max(gower_mat), arr.ind = TRUE)[1, ], ]
```


### Use PAM (partitioning around medoids) to perform the clustering
Calculate Silhouette width for 2 to 10 clusters using PAM

```{r}
sil <- c(NA)
for(i in 2:10){
  pam_fit <- pam(gower_mat, diss=TRUE, k=i)
  sil[i] <-pam_fit$silinfo$avg.width
}
```


```{r}
plot(1:10, sil,
xlab = "Number of clusters",
ylab = "Silhouette Width",
lines(1:10, sil))
```
Plot silhouette width (higher is better, clusters = 2 or 3 has the highest sil value; 4 and 5 are close seconds.)




Cluster Interpretation: via Descriptive Statistics
```{r}
pam_fit <- pam(gower_dist, diss = TRUE, k = 7)
# add cluster labels to the data. We will use result1 later
data_clean <- data.frame(data, pam_fit$cluster)
# show clustering results by country
result1 <- data_clean %>% select(Country,pam_fit.cluster)
View(result1)
clusplot(pam_fit)

```

## 1.run Kmean or PAM on your dataset (run it multiple times without setting set.seed()). Report differences in performance between runs (Silhouette width or overlaps of observations in different clustering). Can you make sense of the clusters produced?

Our data shows a lot of overlapping variables within the 3 clusters. As we increase the distance, the point variability lowers and spreads the data out more, providing more sparse data making it harder to view "clear" clusters.


the 91.31% of point vairability showing the information of the 3 clusters on a 2d scale.
```{r}
<<<<<<< HEAD
library(cluster)
#row.names(result1) = 'Country'
PAM3 <- pam(data[3:21],3)
=======
PAM3 <- pam(data[2:4],3)
>>>>>>> fa26c8293a3ac213580490f2ffa7a3495bd8b517
clusplot(PAM3, color=TRUE, shade=T, labels =3, lines=0)

```

Another run of the data with differnt distance measures shows a lower point variability at 63.68%
```{r}
PAM3 <- pam(data[1:5],3)
clusplot(PAM3, color=TRUE, shade=T, labels =3, lines=0)
```

Two clusters doesn't change much, just in that now there are two clusters.
```{r}
PAM3 <- pam(data[2:4],2)
clusplot(PAM3, color=TRUE, shade=T, labels =3, lines=0)
```

A slightly smaller distance interval [2:5] shows a higher point variability than before, but lower than the first instance of [2:4]
```{r}
PAM3 <- pam(data[2:5],3)
clusplot(PAM3, color=TRUE, shade=T, labels =3, lines=0)
```

A plot with all numeric columns 
```{r}
PAM3 <- pam(data[3:16],3)
clusplot(PAM3, color=TRUE, shade=T, labels =3, lines=0)
```

group_by cluster and then compute the summary data (means, median, etc) for each cluster
```{r}
data_results <- data_clean %>%
  mutate(cluster = pam_fit.cluster) %>% #add the cluster column
  select(-pam_fit.cluster) %>% 
  group_by(cluster) %>% #group countries by its cluster 
  do(the_summary = summary(.)) #do: summarize by group/cluster,add the_summary column

data_results$the_summary
```
The results suggest

_Cluster 1_:Developing countries in Asia with a LE of 68 with a mean GDP of 1,283.  
_Cluster 2_: Developing countries in Europe with a mean LE of 76 with a mean GDP of 9,872.  
_Cluster 3_: Developing countries in Africa with a mean LE of 61 with a mean GDP of 1,962.   
_Cluster 4_: Developing countries in the Americas with a mean LE of 74 with a mean GDP of 7,347.  
_Cluster 5_: Developing countries in Asia with a mean LE of 72 with a mean GDP of 5,052.  
_Cluster 6_: Developed countries in Europe and Asia with a mean LE of 80 with a mean GDP of 20,710.  
_Cluster 7_: Developing countries in Oceania with a mean LE of 69 with a mean GDP of 2,651.  

Output medoids: representative country in each cluster:
```{r}
data[pam_fit$medoids, ]
```



One way to visualize many variables in a lower dimensional space is with t-distributed stochastic neighborhood embedding, or t-SNE. 
This method is a dimension reduction technique that tries to preserve local structure so as to make clusters visible in a 2D or 3D visualization.
it is a powerful but also sometimes puzzling technique, more see https://distill.pub/2016/misread-tsne/ 
```{r}
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>% 
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering),
         name = data_clean$Country)

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster)) + 
  geom_text(data=subset(tsne_data[pam_fit$medoids, ]), # add names of centroids in 
            aes(X,Y,label=name))
```


## cluster the same data set using kmeans
different implementations of k-means are provided throught the algorithm argument to 
kmeans() method for all country attrubutes

To know more about the differences see https://core.ac.uk/download/pdf/27210461.pdf 
kmeans() take numeric data, 

```{r}

# z-score transformation to scale the variables. Scaling is needed because we will use euclidean distant
data_scaled <- data %>% 
  select(-c(Status,region)) %>%  # remove factor variables
  mutate_at(scale, .vars=vars(-Country))
data_scaled %>% head()

```

get distance matrix, excluding first column: Country

note: nstart is the parameter that allows the user to try multiple sets of initial centroids. You should use a nstart > 1, for example, nstart=25, this will run kmeans nstart number of times, each time with a different set of initial centroids. kmeans will then select the one with the lowest within cluster variation.
```{r}
dist_mat <- dist(data_scaled[, -1], method="euclidean")
rownames


avgS <- c() #initiate an empty vector

for(k in 2:10){
  kmeans_cl <- kmeans(data_scaled[,-1], centers=k, iter.max=500, nstart=1)
  s <- silhouette(kmeans_cl$cluster, dist_mat)
  avgS <- c(avgS, mean(s[,3])) # take the mean of sil_width of all observations, and save it in the avgS vector
}

data.frame(nClus=2:10, Silh=avgS)
plot(2:10, avgS,
  xlab = "Number of clusters",
  ylab = "Silhouette Width",
  lines(2:10, avgS))
```


2 or 3 clusters seem to be the best, with marginal differences between them

```{r}
kmeans_fit <- kmeans(data_scaled[,-1], 2)
```

get cluster means
```{r}
aggregate(data_scaled[,-1],by=list(kmeans_fit$cluster),FUN=mean)
``` 

append cluster assignment to data. As seen in the clusters, the 4th cluster is mostly useless. 
```{r}
data_scaled <- data.frame(data_scaled, kmeans_fit$cluster)

# Cluster Interpretation:via visualization
tsne_obj <- Rtsne(dist_mat, is_distance = TRUE)
tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(kmeans_fit$cluster),
         name = data_scaled$name) %>% 
  ggplot(aes(x = X, y = Y)) +
  geom_point(aes(color = cluster))
```

Now try 7 clusters
```{r}
kmeans_fit <- kmeans(data_scaled[,-1], 7)
```

get cluster means
```{r}
aggregate(data_scaled[,-1],by=list(kmeans_fit$cluster),FUN=mean)
``` 

append cluster assignment to data. As seen in the clusters, the 4th cluster is mostly useless. 
```{r}
data_scaled <- data.frame(data_scaled, kmeans_fit$cluster)

# Cluster Interpretation:via visualization
tsne_obj <- Rtsne(dist_mat, is_distance = TRUE)
tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(kmeans_fit$cluster),
         name = data_scaled$name) %>% 
  ggplot(aes(x = X, y = Y)) +
  geom_point(aes(color = cluster))
```

And just for fun, 9 to coincide with our silhouette graph.
```{r}
kmeans_fit <- kmeans(data_scaled[,-1], 9)
```

get cluster means
```{r}
aggregate(data_scaled[,-1],by=list(kmeans_fit$cluster),FUN=mean)
``` 

append cluster assignment to data. As seen in the clusters, the 4th cluster is mostly useless. 
```{r}
data_scaled <- data.frame(data_scaled, kmeans_fit$cluster)

# Cluster Interpretation:via visualization
tsne_obj <- Rtsne(dist_mat, is_distance = TRUE)
tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(kmeans_fit$cluster),
         name = data_scaled$name) %>% 
  ggplot(aes(x = X, y = Y)) +
  geom_point(aes(color = cluster))
```
After trying kmeans multiple times, the optimum clusters seem to be two, all other clusters are outliers, which might prove interesting with future research. 



##2.Run a hierarchical method on your dataset, try a couple of different cuts. Are these results more meaningful than those from the participation method?


>method argument takes the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of:

>"ward.D", "ward.D2", "single", "complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC).

```{r}
h <- hclust(d = as.dist(gower_dist), method="complete")
# plot the dendrogram #hang decides where to start labels on axis. -0.1 is negative, so the x-axis labels will hang down from 0
plot(h, hang=-0.1)
```

a nicer dengrogram (kind of) since the last one doesn't have labels
```{r}
data_dendro <- data %>% select(-c(region, Status,Country)) %>% scale()
rownames(data_dendro) <- data$Country
data_dendro <- dist(data_dendro, method="manhattan")
data_dendro <- as.dendrogram(hclust(data_dendro, method="ward.D2"))
dendro.plot <- ggdendrogram(data = data_dendro, rotate = TRUE) + 
  theme(axis.text.y = element_text(size = 8)) 
print(dendro.plot) #better plot
```

If we want to cluster at a height of about 0.3 is shows 6 clusters at the 3rd level. They seem to be evenly spread except cluster 6. 
```{r}
clus <- cutree(h, h=.3)
table(clus)
```

At just 3 clusters we find a disproportionate amount of variables per cluster.
```{r}
clus3 <- cutree(h, 3)
table(clus3)
```

the height of .4 is similar to .3, where two of the clusters were now combined.
```{r}
clus <- cutree(h, h=.4)
table(clus)
```

Four clusters are exactly the same as a .4 height.
```{r}
clus <- cutree(h, 4)
table(clus)
```


```{r}
clus <- cutree(h, h=.2)
#plot(clus)
table(clus)
```

```{r}
clus <- cutree(h, h=.5)
table(clus)
```

As expected, the lower height you go (.2), the more clusters with a disperesed amount of variables in each. Whereas if you go to a higher height (.5) the less clusters you get with very different variables in each.


```{r}
clus <- cutree(h, 7)
table(clus)
```

compare with pam 3-cluter results 
```{r}
table(result1$pam_fit.cluster, clus)
```



##use DIANA in the cluster package for divisive hierarchical clustering
because we provided a dissimilarity matrix, we don't need to supply other arguments to this function.
dissimilarity matrix incoporated the distance measures and data normalization
```{r}
d <- diana(gower_dist)
d7 <- cutree(d, 7)

# visualize Diana
plot(d, which = 2, nmax.lab = 100)

table(result1$pam_fit.cluster, d7)
table(clus, d7) # versus the hclust from above

```


















